{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Sentiment Analysis of Movie Reviews\n\n**Objective**: Build a sentiment analysis model to classify IMDb movie reviews as positive or negative.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nltk\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:44:42.972462Z","iopub.execute_input":"2024-04-13T16:44:42.972980Z","iopub.status.idle":"2024-04-13T16:45:00.292064Z","shell.execute_reply.started":"2024-04-13T16:44:42.972946Z","shell.execute_reply":"2024-04-13T16:45:00.290656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There was an issue accessing the IMDb movie review dataset. We used an alternative method to load the dataset `nltk` library, which provides access to the IMDb movie reviews dataset.","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import movie_reviews\n\n# Download the IMDb movie reviews dataset\nnltk.download('movie_reviews')\n\n# Load the movie reviews\nreviews = []\nfor category in movie_reviews.categories():\n    for fileid in movie_reviews.fileids(category):\n        review = {\n            'text': movie_reviews.raw(fileid),\n            'label': category\n        }\n        reviews.append(review)\n\n# Convert to DataFrame\nimdb_df = pd.DataFrame(reviews)\n\n# Display the first few rows of the dataset\nprint(imdb_df.head())\n\n# Check the distribution of labels\nprint(imdb_df['label'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:00:12.385918Z","iopub.execute_input":"2024-04-13T18:00:12.386786Z","iopub.status.idle":"2024-04-13T18:00:12.719227Z","shell.execute_reply.started":"2024-04-13T18:00:12.386745Z","shell.execute_reply":"2024-04-13T18:00:12.718186Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package movie_reviews is already up-to-date!\n                                                text label\n0  plot : two teen couples go to a church party ,...   neg\n1  the happy bastard's quick movie review \\ndamn ...   neg\n2  it is movies like these that make a jaded movi...   neg\n3   \" quest for camelot \" is warner bros . ' firs...   neg\n4  synopsis : a mentally unstable man undergoing ...   neg\nlabel\nneg    1000\npos    1000\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Text Preprocessing ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport string\n\n# Download stopwords\nnltk.download('stopwords')\n\n# Initialize stemmer and set of stopwords\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\n\n# Function to preprocess text\ndef preprocess_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Removing special characters and stopwords, and performing stemming\n    preprocessed_tokens = []\n    for token in tokens:\n        if token not in string.punctuation and token not in stop_words:\n            preprocessed_tokens.append(stemmer.stem(token))\n    \n    # Join tokens back into text\n    preprocessed_text = ' '.join(preprocessed_tokens)\n    \n    return preprocessed_text\n\n# Apply text preprocessing to the 'text' column of the DataFrame\nimdb_df['preprocessed_text'] = imdb_df['text'].apply(preprocess_text)\n\n# Display the preprocessed text\nprint(imdb_df['preprocessed_text'].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:02:13.552964Z","iopub.execute_input":"2024-04-13T18:02:13.553410Z","iopub.status.idle":"2024-04-13T18:03:02.598628Z","shell.execute_reply.started":"2024-04-13T18:02:13.553381Z","shell.execute_reply":"2024-04-13T18:03:02.597099Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n0    plot two teen coupl go church parti drink driv...\n1    happi bastard 's quick movi review damn y2k bu...\n2    movi like make jade movi viewer thank invent t...\n3    `` quest camelot `` warner bro first feature-l...\n4    synopsi mental unstabl man undergo psychothera...\nName: preprocessed_text, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## TF-IDF Feature Extraction ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize the TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\n\n# Fit and transform the preprocessed text data\nX_tfidf = tfidf_vectorizer.fit_transform(imdb_df['preprocessed_text'])\n\n# Display the shape of the TF-IDF matrix\nprint(\"TF-IDF matrix shape:\", X_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:03:04.755147Z","iopub.execute_input":"2024-04-13T18:03:04.756486Z","iopub.status.idle":"2024-04-13T18:03:05.773469Z","shell.execute_reply.started":"2024-04-13T18:03:04.756449Z","shell.execute_reply":"2024-04-13T18:03:05.772302Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"TF-IDF matrix shape: (2000, 27273)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Selection ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, imdb_df['label'], test_size=0.2, random_state=42)\n\n# Initialize the logistic regression model\nlogistic_regression_model = LogisticRegression()\n\n# Train the model on the training data\nlogistic_regression_model.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:03:30.958421Z","iopub.execute_input":"2024-04-13T18:03:30.959692Z","iopub.status.idle":"2024-04-13T18:03:31.544425Z","shell.execute_reply.started":"2024-04-13T18:03:30.959641Z","shell.execute_reply":"2024-04-13T18:03:31.542804Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Evaluation ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Make predictions on the test data\ny_pred = logistic_regression_model.predict(X_test)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, pos_label='pos')\nrecall = recall_score(y_test, y_pred, pos_label='pos')\nf1 = f1_score(y_test, y_pred, pos_label='pos')\n\n# Print the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-Score:\", f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:03:40.791670Z","iopub.execute_input":"2024-04-13T18:03:40.792157Z","iopub.status.idle":"2024-04-13T18:03:40.827126Z","shell.execute_reply.started":"2024-04-13T18:03:40.792122Z","shell.execute_reply":"2024-04-13T18:03:40.825921Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 0.805\nPrecision: 0.8\nRecall: 0.8159203980099502\nF1-Score: 0.8078817733990147\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Tuning (Optional)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameters grid\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n\n# Initialize the grid search with cross-validation\ngrid_search = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)\n\n# Perform grid search\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Evaluate the model with the best hyperparameters\nbest_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test)\naccuracy_best = accuracy_score(y_test, y_pred_best)\nprecision_best = precision_score(y_test, y_pred_best, pos_label='pos')\nrecall_best = recall_score(y_test, y_pred_best, pos_label='pos')\nf1_best = f1_score(y_test, y_pred_best, pos_label='pos')\n\n# Print the evaluation metrics for the best model\nprint(\"Accuracy (Best Model):\", accuracy_best)\nprint(\"Precision (Best Model):\", precision_best)\nprint(\"Recall (Best Model):\", recall_best)\nprint(\"F1-Score (Best Model):\", f1_best)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:03:48.188482Z","iopub.execute_input":"2024-04-13T18:03:48.188958Z","iopub.status.idle":"2024-04-13T18:04:02.107855Z","shell.execute_reply.started":"2024-04-13T18:03:48.188925Z","shell.execute_reply":"2024-04-13T18:04:02.106188Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n25 fits failed out of a total of 50.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [     nan 0.561875      nan 0.77625       nan 0.800625      nan 0.821875\n      nan 0.8375  ]\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Best Hyperparameters: {'C': 10, 'penalty': 'l2'}\nAccuracy (Best Model): 0.8375\nPrecision (Best Model): 0.8366336633663366\nRecall (Best Model): 0.8407960199004975\nF1-Score (Best Model): 0.8387096774193548\n","output_type":"stream"}]},{"cell_type":"markdown","source":"There were some failures during the grid search process, likely due to incompatible hyperparameter combinations. However, despite these failures, we still managed to find the best hyperparameters for the logistic regression model.\n\nHere's the result of the hyperparameter tuning:\n\n**Best Hyperparameters: {'C': 10, 'penalty': 'l2'}\nAccuracy (Best Model): 0.8375\nPrecision (Best Model): 0.8366\nRecall (Best Model): 0.8408\nF1-Score (Best Model): 0.8387**","metadata":{}},{"cell_type":"markdown","source":"## Prediction ","metadata":{}},{"cell_type":"code","source":"# Example text for prediction\nnew_reviews = [\n    \"This movie was amazing! I loved every moment of it.\",\n    \"The acting was terrible and the plot was boring.\",\n    \"I couldn't stop laughing throughout the entire film.\"\n]\n\n# Preprocess the new reviews\npreprocessed_new_reviews = [preprocess_text(review) for review in new_reviews]\n\n# Convert the preprocessed text into TF-IDF features\nX_new_tfidf = tfidf_vectorizer.transform(preprocessed_new_reviews)\n\n# Make predictions on the new reviews\npredictions = logistic_regression_model.predict(X_new_tfidf)\n\n# Print the predictions\nfor review, prediction in zip(new_reviews, predictions):\n    print(\"Review:\", review)\n    print(\"Predicted Sentiment:\", prediction)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:04:38.308623Z","iopub.execute_input":"2024-04-13T18:04:38.309601Z","iopub.status.idle":"2024-04-13T18:04:38.326386Z","shell.execute_reply.started":"2024-04-13T18:04:38.309542Z","shell.execute_reply":"2024-04-13T18:04:38.324623Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Review: This movie was amazing! I loved every moment of it.\nPredicted Sentiment: pos\n\nReview: The acting was terrible and the plot was boring.\nPredicted Sentiment: neg\n\nReview: I couldn't stop laughing throughout the entire film.\nPredicted Sentiment: pos\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Conclusion ","metadata":{}},{"cell_type":"markdown","source":"After completing the sentiment analysis of IMDb movie reviews using a logistic regression model, we achieved the following results:\n\nThe logistic regression model achieved an accuracy of approximately 83.75% on the test data, indicating that it can effectively classify movie reviews as positive or negative.\nThe precision, recall, and F1-score of the model were around 0.84, indicating a good balance between precision (ability to avoid false positives) and recall (ability to identify positive samples).\nInsights from the analysis:\n\nThe logistic regression model, trained on TF-IDF features, performed well in classifying movie reviews as positive or negative, demonstrating the effectiveness of text classification techniques in sentiment analysis tasks.\nThe model's performance could potentially be further improved with additional preprocessing steps, feature engineering, or experimenting with different machine learning algorithms.\n\nOverall, the sentiment analysis experiment provided valuable insights into the sentiment of IMDb movie reviews and demonstrated the effectiveness of machine learning models in analyzing text data.","metadata":{}}]}