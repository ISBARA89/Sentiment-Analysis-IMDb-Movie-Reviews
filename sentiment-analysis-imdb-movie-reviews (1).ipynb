{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-13T16:42:44.738562Z","iopub.execute_input":"2024-04-13T16:42:44.738990Z","iopub.status.idle":"2024-04-13T16:42:46.148375Z","shell.execute_reply.started":"2024-04-13T16:42:44.738958Z","shell.execute_reply":"2024-04-13T16:42:46.146622Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Analysis of Movie Reviews\n\n**Objective**: Build a sentiment analysis model to classify IMDb movie reviews as positive or negative.","metadata":{}},{"cell_type":"code","source":"pip install nltk\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:44:42.972462Z","iopub.execute_input":"2024-04-13T16:44:42.972980Z","iopub.status.idle":"2024-04-13T16:45:00.292064Z","shell.execute_reply.started":"2024-04-13T16:44:42.972946Z","shell.execute_reply":"2024-04-13T16:45:00.290656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There was an issue accessing the IMDb movie review dataset. We used an alternative method to load the dataset `nltk` library, which provides access to the IMDb movie reviews dataset.","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import movie_reviews\n\n# Download the IMDb movie reviews dataset\nnltk.download('movie_reviews')\n\n# Load the movie reviews\nreviews = []\nfor category in movie_reviews.categories():\n    for fileid in movie_reviews.fileids(category):\n        review = {\n            'text': movie_reviews.raw(fileid),\n            'label': category\n        }\n        reviews.append(review)\n\n# Convert to DataFrame\nimdb_df = pd.DataFrame(reviews)\n\n# Display the first few rows of the dataset\nprint(imdb_df.head())\n\n# Check the distribution of labels\nprint(imdb_df['label'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:45:11.902547Z","iopub.execute_input":"2024-04-13T16:45:11.902976Z","iopub.status.idle":"2024-04-13T16:45:15.881023Z","shell.execute_reply.started":"2024-04-13T16:45:11.902938Z","shell.execute_reply":"2024-04-13T16:45:15.879237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Preprocessing ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport string\n\n# Download stopwords\nnltk.download('stopwords')\n\n# Initialize stemmer and set of stopwords\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\n\n# Function to preprocess text\ndef preprocess_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Removing special characters and stopwords, and performing stemming\n    preprocessed_tokens = []\n    for token in tokens:\n        if token not in string.punctuation and token not in stop_words:\n            preprocessed_tokens.append(stemmer.stem(token))\n    \n    # Join tokens back into text\n    preprocessed_text = ' '.join(preprocessed_tokens)\n    \n    return preprocessed_text\n\n# Apply text preprocessing to the 'text' column of the DataFrame\nimdb_df['preprocessed_text'] = imdb_df['text'].apply(preprocess_text)\n\n# Display the preprocessed text\nprint(imdb_df['preprocessed_text'].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:46:16.122295Z","iopub.execute_input":"2024-04-13T16:46:16.123651Z","iopub.status.idle":"2024-04-13T16:47:04.995163Z","shell.execute_reply.started":"2024-04-13T16:46:16.123597Z","shell.execute_reply":"2024-04-13T16:47:04.994144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF Feature Extraction ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize the TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\n\n# Fit and transform the preprocessed text data\nX_tfidf = tfidf_vectorizer.fit_transform(imdb_df['preprocessed_text'])\n\n# Display the shape of the TF-IDF matrix\nprint(\"TF-IDF matrix shape:\", X_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:48:29.027435Z","iopub.execute_input":"2024-04-13T16:48:29.028021Z","iopub.status.idle":"2024-04-13T16:48:30.139793Z","shell.execute_reply.started":"2024-04-13T16:48:29.027979Z","shell.execute_reply":"2024-04-13T16:48:30.138587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf, imdb_df['label'], test_size=0.2, random_state=42)\n\n# Initialize the logistic regression model\nlogistic_regression_model = LogisticRegression()\n\n# Train the model on the training data\nlogistic_regression_model.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:49:54.571855Z","iopub.execute_input":"2024-04-13T16:49:54.572343Z","iopub.status.idle":"2024-04-13T16:49:55.428403Z","shell.execute_reply.started":"2024-04-13T16:49:54.572311Z","shell.execute_reply":"2024-04-13T16:49:55.425971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Make predictions on the test data\ny_pred = logistic_regression_model.predict(X_test)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, pos_label='pos')\nrecall = recall_score(y_test, y_pred, pos_label='pos')\nf1 = f1_score(y_test, y_pred, pos_label='pos')\n\n# Print the evaluation metrics\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-Score:\", f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:51:27.582457Z","iopub.execute_input":"2024-04-13T16:51:27.584169Z","iopub.status.idle":"2024-04-13T16:51:27.621757Z","shell.execute_reply.started":"2024-04-13T16:51:27.584105Z","shell.execute_reply":"2024-04-13T16:51:27.620297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Tuning (Optional)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameters grid\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n\n# Initialize the grid search with cross-validation\ngrid_search = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)\n\n# Perform grid search\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Evaluate the model with the best hyperparameters\nbest_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test)\naccuracy_best = accuracy_score(y_test, y_pred_best)\nprecision_best = precision_score(y_test, y_pred_best, pos_label='pos')\nrecall_best = recall_score(y_test, y_pred_best, pos_label='pos')\nf1_best = f1_score(y_test, y_pred_best, pos_label='pos')\n\n# Print the evaluation metrics for the best model\nprint(\"Accuracy (Best Model):\", accuracy_best)\nprint(\"Precision (Best Model):\", precision_best)\nprint(\"Recall (Best Model):\", recall_best)\nprint(\"F1-Score (Best Model):\", f1_best)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:52:15.442354Z","iopub.execute_input":"2024-04-13T16:52:15.443303Z","iopub.status.idle":"2024-04-13T16:52:30.610182Z","shell.execute_reply.started":"2024-04-13T16:52:15.443252Z","shell.execute_reply":"2024-04-13T16:52:30.608093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There were some failures during the grid search process, likely due to incompatible hyperparameter combinations. However, despite these failures, we still managed to find the best hyperparameters for the logistic regression model.\n\nHere's the result of the hyperparameter tuning:\n\n**Best Hyperparameters: {'C': 10, 'penalty': 'l2'}\nAccuracy (Best Model): 0.8375\nPrecision (Best Model): 0.8366\nRecall (Best Model): 0.8408\nF1-Score (Best Model): 0.8387**","metadata":{}},{"cell_type":"markdown","source":"## Prediction ","metadata":{}},{"cell_type":"code","source":"# Example text for prediction\nnew_reviews = [\n    \"This movie was amazing! I loved every moment of it.\",\n    \"The acting was terrible and the plot was boring.\",\n    \"I couldn't stop laughing throughout the entire film.\"\n]\n\n# Preprocess the new reviews\npreprocessed_new_reviews = [preprocess_text(review) for review in new_reviews]\n\n# Convert the preprocessed text into TF-IDF features\nX_new_tfidf = tfidf_vectorizer.transform(preprocessed_new_reviews)\n\n# Make predictions on the new reviews\npredictions = logistic_regression_model.predict(X_new_tfidf)\n\n# Print the predictions\nfor review, prediction in zip(new_reviews, predictions):\n    print(\"Review:\", review)\n    print(\"Predicted Sentiment:\", prediction)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:57:22.794242Z","iopub.execute_input":"2024-04-13T16:57:22.794772Z","iopub.status.idle":"2024-04-13T16:57:22.810618Z","shell.execute_reply.started":"2024-04-13T16:57:22.794738Z","shell.execute_reply":"2024-04-13T16:57:22.809027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion ","metadata":{}},{"cell_type":"markdown","source":"After completing the sentiment analysis of IMDb movie reviews using a logistic regression model, we achieved the following results:\n\nThe logistic regression model achieved an accuracy of approximately 83.75% on the test data, indicating that it can effectively classify movie reviews as positive or negative.\nThe precision, recall, and F1-score of the model were around 0.84, indicating a good balance between precision (ability to avoid false positives) and recall (ability to identify positive samples).\nInsights from the analysis:\n\nThe logistic regression model, trained on TF-IDF features, performed well in classifying movie reviews as positive or negative, demonstrating the effectiveness of text classification techniques in sentiment analysis tasks.\nThe model's performance could potentially be further improved with additional preprocessing steps, feature engineering, or experimenting with different machine learning algorithms.\n\nOverall, the sentiment analysis experiment provided valuable insights into the sentiment of IMDb movie reviews and demonstrated the effectiveness of machine learning models in analyzing text data.","metadata":{}}]}